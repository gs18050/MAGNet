{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo8tcKT0ItFe"
      },
      "source": [
        "# To start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlb8bRR8ItFf"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import random\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "import torchvision.models as models\n",
        "import math\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torchvision.transforms.functional import InterpolationMode as IMode\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeauWSOmVkQr",
        "outputId": "f1803468-236d-4a10-ea48-19b0e4433e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 사용 가능 여부: True\n",
            "사용 중인 GPU 이름: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"GPU 사용 가능 여부:\", torch.cuda.is_available())\n",
        "print(\"사용 중인 GPU 이름:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"GPU가 사용 중이 아닙니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN7bXoGIJTX1",
        "outputId": "3f051775-cb93-4cc8-9ed5-80661607a3de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/sryeo\n"
          ]
        }
      ],
      "source": [
        "# For when using google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_root='/content/drive/MyDrive/MAGNet' # Put your own path\n",
        "print(drive_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6oaIdrmItFg"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhPwGdxkItFg"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnMBVuOTItFg"
      },
      "outputs": [],
      "source": [
        "# from torch import nn as nn\n",
        "# from torch.nn import functional as F\n",
        "# from torch.nn.utils import spectral_norm\n",
        "\n",
        "# from basicsr.utils.registry import ARCH_REGISTRY\n",
        "\n",
        "class VGGStyleDiscriminator(nn.Module):\n",
        "    \"\"\"VGG style discriminator with input size 128 x 128 or 256 x 256.\n",
        "\n",
        "    It is used to train SRGAN, ESRGAN, and VideoGAN.\n",
        "\n",
        "    Args:\n",
        "        num_in_ch (int): Channel number of inputs. Default: 3.\n",
        "        num_feat (int): Channel number of base intermediate features.Default: 64.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_in_ch, num_feat, input_size=128):\n",
        "        super(VGGStyleDiscriminator, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        assert self.input_size == 128 or self.input_size == 256, (\n",
        "            f'input size must be 128 or 256, but received {input_size}')\n",
        "\n",
        "        self.conv0_0 = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1, bias=True)\n",
        "        self.conv0_1 = nn.Conv2d(num_feat, num_feat, 4, 2, 1, bias=False)\n",
        "        self.bn0_1 = nn.BatchNorm2d(num_feat, affine=True)\n",
        "\n",
        "        self.conv1_0 = nn.Conv2d(num_feat, num_feat * 2, 3, 1, 1, bias=False)\n",
        "        self.bn1_0 = nn.BatchNorm2d(num_feat * 2, affine=True)\n",
        "        self.conv1_1 = nn.Conv2d(num_feat * 2, num_feat * 2, 4, 2, 1, bias=False)\n",
        "        self.bn1_1 = nn.BatchNorm2d(num_feat * 2, affine=True)\n",
        "\n",
        "        self.conv2_0 = nn.Conv2d(num_feat * 2, num_feat * 4, 3, 1, 1, bias=False)\n",
        "        self.bn2_0 = nn.BatchNorm2d(num_feat * 4, affine=True)\n",
        "        self.conv2_1 = nn.Conv2d(num_feat * 4, num_feat * 4, 4, 2, 1, bias=False)\n",
        "        self.bn2_1 = nn.BatchNorm2d(num_feat * 4, affine=True)\n",
        "\n",
        "        self.conv3_0 = nn.Conv2d(num_feat * 4, num_feat * 8, 3, 1, 1, bias=False)\n",
        "        self.bn3_0 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
        "        self.conv3_1 = nn.Conv2d(num_feat * 8, num_feat * 8, 4, 2, 1, bias=False)\n",
        "        self.bn3_1 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
        "\n",
        "        self.conv4_0 = nn.Conv2d(num_feat * 8, num_feat * 8, 3, 1, 1, bias=False)\n",
        "        self.bn4_0 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
        "        self.conv4_1 = nn.Conv2d(num_feat * 8, num_feat * 8, 4, 2, 1, bias=False)\n",
        "        self.bn4_1 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
        "\n",
        "        if self.input_size == 256:\n",
        "            self.conv5_0 = nn.Conv2d(num_feat * 8, num_feat * 8, 3, 1, 1, bias=False)\n",
        "            self.bn5_0 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
        "            self.conv5_1 = nn.Conv2d(num_feat * 8, num_feat * 8, 4, 2, 1, bias=False)\n",
        "            self.bn5_1 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
        "\n",
        "        self.linear1 = nn.Linear(num_feat * 8 * 4 * 4, 80)\n",
        "        self.linear2 = nn.Linear(80, 1)\n",
        "\n",
        "        # activation function\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(2) == self.input_size, (f'Input size must be identical to input_size, but received {x.size()}.')\n",
        "\n",
        "        feat = self.lrelu(self.conv0_0(x))\n",
        "        feat = self.lrelu(self.bn0_1(self.conv0_1(feat)))  # output spatial size: /2\n",
        "\n",
        "        feat = self.lrelu(self.bn1_0(self.conv1_0(feat)))\n",
        "        feat = self.lrelu(self.bn1_1(self.conv1_1(feat)))  # output spatial size: /4\n",
        "\n",
        "        feat = self.lrelu(self.bn2_0(self.conv2_0(feat)))\n",
        "        feat = self.lrelu(self.bn2_1(self.conv2_1(feat)))  # output spatial size: /8\n",
        "\n",
        "        feat = self.lrelu(self.bn3_0(self.conv3_0(feat)))\n",
        "        feat = self.lrelu(self.bn3_1(self.conv3_1(feat)))  # output spatial size: /16\n",
        "\n",
        "        feat = self.lrelu(self.bn4_0(self.conv4_0(feat)))\n",
        "        feat = self.lrelu(self.bn4_1(self.conv4_1(feat)))  # output spatial size: /32\n",
        "\n",
        "        if self.input_size == 256:\n",
        "            feat = self.lrelu(self.bn5_0(self.conv5_0(feat)))\n",
        "            feat = self.lrelu(self.bn5_1(self.conv5_1(feat)))  # output spatial size: / 64\n",
        "\n",
        "        # spatial size: (4, 4)\n",
        "        feat = feat.view(feat.size(0), -1)\n",
        "        feat = self.lrelu(self.linear1(feat))\n",
        "        out = self.linear2(feat)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJfYO0pPneCE"
      },
      "source": [
        "### GANLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_3Xw2t1neCE"
      },
      "outputs": [],
      "source": [
        "class GANLoss(nn.Module):\n",
        "    \"\"\"Define GAN loss.\n",
        "\n",
        "    Args:\n",
        "        gan_type (str): Support 'vanilla', 'lsgan', 'wgan', 'hinge'.\n",
        "        real_label_val (float): The value for real label. Default: 1.0.\n",
        "        fake_label_val (float): The value for fake label. Default: 0.0.\n",
        "        loss_weight (float): Loss weight. Default: 1.0.\n",
        "            Note that loss_weight is only for generators; and it is always 1.0\n",
        "            for discriminators.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0, loss_weight=1.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.gan_type = gan_type\n",
        "        self.loss_weight = loss_weight\n",
        "        self.real_label_val = real_label_val\n",
        "        self.fake_label_val = fake_label_val\n",
        "\n",
        "        if self.gan_type == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif self.gan_type == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif self.gan_type == 'wgan':\n",
        "            self.loss = self._wgan_loss\n",
        "        elif self.gan_type == 'wgan_softplus':\n",
        "            self.loss = self._wgan_softplus_loss\n",
        "        elif self.gan_type == 'hinge':\n",
        "            self.loss = nn.ReLU()\n",
        "        else:\n",
        "            raise NotImplementedError(f'GAN type {self.gan_type} is not implemented.')\n",
        "\n",
        "    def _wgan_loss(self, input, target):\n",
        "        \"\"\"wgan loss.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Input tensor.\n",
        "            target (bool): Target label.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: wgan loss.\n",
        "        \"\"\"\n",
        "        return -input.mean() if target else input.mean()\n",
        "\n",
        "    def _wgan_softplus_loss(self, input, target):\n",
        "        \"\"\"wgan loss with soft plus. softplus is a smooth approximation to the\n",
        "        ReLU function.\n",
        "\n",
        "        In StyleGAN2, it is called:\n",
        "            Logistic loss for discriminator;\n",
        "            Non-saturating loss for generator.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Input tensor.\n",
        "            target (bool): Target label.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: wgan loss.\n",
        "        \"\"\"\n",
        "        return F.softplus(-input).mean() if target else F.softplus(input).mean()\n",
        "\n",
        "    def get_target_label(self, input, target_is_real):\n",
        "        \"\"\"Get target label.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Input tensor.\n",
        "            target_is_real (bool): Whether the target is real or fake.\n",
        "\n",
        "        Returns:\n",
        "            (bool | Tensor): Target tensor. Return bool for wgan, otherwise,\n",
        "                return Tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.gan_type in ['wgan', 'wgan_softplus']:\n",
        "            return target_is_real\n",
        "        target_val = (self.real_label_val if target_is_real else self.fake_label_val)\n",
        "        return input.new_ones(input.size()) * target_val\n",
        "\n",
        "    def forward(self, input, target_is_real, is_disc=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input (Tensor): The input for the loss module, i.e., the network\n",
        "                prediction.\n",
        "            target_is_real (bool): Whether the targe is real or fake.\n",
        "            is_disc (bool): Whether the loss for discriminators or not.\n",
        "                Default: False.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: GAN loss value.\n",
        "        \"\"\"\n",
        "        target_label = self.get_target_label(input, target_is_real)\n",
        "        if self.gan_type == 'hinge':\n",
        "            if is_disc:  # for discriminators in hinge-gan\n",
        "                input = -input if target_is_real else input\n",
        "                loss = self.loss(1 + input).mean()\n",
        "            else:  # for generators in hinge-gan\n",
        "                loss = -input.mean()\n",
        "        else:  # other gan types\n",
        "            loss = self.loss(input, target_label)\n",
        "\n",
        "        # loss_weight is always 1.0 for discriminators\n",
        "        return loss if is_disc else loss * self.loss_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-IMxoDrneCF"
      },
      "source": [
        "### VGG Extract Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYqWx32ineCF"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from collections import OrderedDict\n",
        "# from torch import nn as nn\n",
        "\n",
        "# from basicsr.utils.registry import ARCH_REGISTRY\n",
        "\n",
        "VGG_PRETRAIN_PATH = 'experiments/pretrained_models/vgg19-dcbb9e9d.pth'\n",
        "NAMES = {\n",
        "    'vgg19': [\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3', 'conv4_1',\n",
        "        'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4', 'conv5_1', 'relu5_1',\n",
        "        'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5'\n",
        "    ]\n",
        "}\n",
        "\n",
        "def insert_bn(names):\n",
        "    \"\"\"Insert bn layer after each conv.\n",
        "\n",
        "    Args:\n",
        "        names (list): The list of layer names.\n",
        "\n",
        "    Returns:\n",
        "        list: The list of layer names with bn layers.\n",
        "    \"\"\"\n",
        "    names_bn = []\n",
        "    for name in names:\n",
        "        names_bn.append(name)\n",
        "        if 'conv' in name:\n",
        "            position = name.replace('conv', '')\n",
        "            names_bn.append('bn' + position)\n",
        "    return names_bn\n",
        "\n",
        "\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    \"\"\"VGG network for feature extraction.\n",
        "\n",
        "    In this implementation, we allow users to choose whether use normalization\n",
        "    in the input feature and the type of vgg network. Note that the pretrained\n",
        "    path must fit the vgg type.\n",
        "\n",
        "    Args:\n",
        "        layer_name_list (list[str]): Forward function returns the corresponding\n",
        "            features according to the layer_name_list.\n",
        "            Example: {'relu1_1', 'relu2_1', 'relu3_1'}.\n",
        "        vgg_type (str): Set the type of vgg network. Default: 'vgg19'.\n",
        "        use_input_norm (bool): If True, normalize the input image. Importantly,\n",
        "            the input feature must in the range [0, 1]. Default: True.\n",
        "        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n",
        "            Default: False.\n",
        "        requires_grad (bool): If true, the parameters of VGG network will be\n",
        "            optimized. Default: False.\n",
        "        remove_pooling (bool): If true, the max pooling operations in VGG net\n",
        "            will be removed. Default: False.\n",
        "        pooling_stride (int): The stride of max pooling operation. Default: 2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 layer_name_list,\n",
        "                 vgg_type='vgg19',\n",
        "                 use_input_norm=True,\n",
        "                 range_norm=False,\n",
        "                 requires_grad=False,\n",
        "                 remove_pooling=False,\n",
        "                 pooling_stride=2):\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.layer_name_list = layer_name_list\n",
        "        self.use_input_norm = use_input_norm\n",
        "        self.range_norm = range_norm\n",
        "\n",
        "        self.names = NAMES[vgg_type.replace('_bn', '')]\n",
        "        if 'bn' in vgg_type:\n",
        "            self.names = insert_bn(self.names)\n",
        "\n",
        "        # only borrow layers that will be used to avoid unused params\n",
        "        max_idx = 0\n",
        "        for v in layer_name_list:\n",
        "            idx = self.names.index(v)\n",
        "            if idx > max_idx:\n",
        "                max_idx = idx\n",
        "\n",
        "        if os.path.exists(VGG_PRETRAIN_PATH):\n",
        "            vgg_net = getattr(models.vgg, vgg_type)(pretrained=False)\n",
        "            state_dict = torch.load(VGG_PRETRAIN_PATH, map_location=lambda storage, loc: storage)\n",
        "            vgg_net.load_state_dict(state_dict)\n",
        "        else:\n",
        "            vgg_net = getattr(models.vgg, vgg_type)(pretrained=True)\n",
        "\n",
        "        features = vgg_net.features[:max_idx + 1]\n",
        "\n",
        "        modified_net = OrderedDict()\n",
        "        for k, v in zip(self.names, features):\n",
        "            if 'pool' in k:\n",
        "                # if remove_pooling is true, pooling operation will be removed\n",
        "                if remove_pooling:\n",
        "                    continue\n",
        "                else:\n",
        "                    # in some cases, we may want to change the default stride\n",
        "                    modified_net[k] = nn.MaxPool2d(kernel_size=2, stride=pooling_stride)\n",
        "            else:\n",
        "                modified_net[k] = v\n",
        "\n",
        "        self.vgg_net = nn.Sequential(modified_net)\n",
        "\n",
        "        if not requires_grad:\n",
        "            self.vgg_net.eval()\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            self.vgg_net.train()\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        if self.use_input_norm:\n",
        "            # the mean is for image with range [0, 1]\n",
        "            self.register_buffer('mean', torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "            # the std is for image with range [0, 1]\n",
        "            self.register_buffer('std', torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor with shape (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Forward results.\n",
        "        \"\"\"\n",
        "        if self.range_norm:\n",
        "            x = (x + 1) / 2\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "\n",
        "        output = {}\n",
        "        for key, layer in self.vgg_net._modules.items():\n",
        "            x = layer(x)\n",
        "            if key in self.layer_name_list:\n",
        "                output[key] = x.clone()\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo8CfRagneCF"
      },
      "source": [
        "### Compaact VGG Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4kXFcvxItFh"
      },
      "outputs": [],
      "source": [
        "class SRVGGNetCompact(nn.Module):\n",
        "    \"\"\"A compact VGG-style network structure for super-resolution.\n",
        "\n",
        "    It is a compact network structure, which performs upsampling in the last layer and no convolution is\n",
        "    conducted on the HR feature space.\n",
        "\n",
        "    Args:\n",
        "        num_in_ch (int): Channel number of inputs. Default: 3.\n",
        "        num_out_ch (int): Channel number of outputs. Default: 3.\n",
        "        num_feat (int): Channel number of intermediate features. Default: 64.\n",
        "        num_conv (int): Number of convolution layers in the body network. Default: 16.\n",
        "        upscale (int): Upsampling factor. Default: 4.\n",
        "        act_type (str): Activation type, options: 'relu', 'prelu', 'leakyrelu'. Default: prelu.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu'):\n",
        "        super(SRVGGNetCompact, self).__init__()\n",
        "        self.num_in_ch = num_in_ch\n",
        "        self.num_out_ch = num_out_ch\n",
        "        self.num_feat = num_feat\n",
        "        self.num_conv = num_conv\n",
        "        self.upscale = upscale\n",
        "        self.act_type = act_type\n",
        "\n",
        "        self.body = nn.ModuleList()\n",
        "        # the first conv\n",
        "        self.body.append(nn.Conv2d(num_in_ch, num_feat, 3, 1, 1))\n",
        "        # the first activation\n",
        "        if act_type == 'relu':\n",
        "            activation = nn.ReLU(inplace=True)\n",
        "        elif act_type == 'prelu':\n",
        "            activation = nn.PReLU(num_parameters=num_feat)\n",
        "        elif act_type == 'leakyrelu':\n",
        "            activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.body.append(activation)\n",
        "\n",
        "        # the body structure\n",
        "        for _ in range(num_conv):\n",
        "            self.body.append(nn.Conv2d(num_feat, num_feat, 3, 1, 1))\n",
        "            # activation\n",
        "            if act_type == 'relu':\n",
        "                activation = nn.ReLU(inplace=True)\n",
        "            elif act_type == 'prelu':\n",
        "                activation = nn.PReLU(num_parameters=num_feat)\n",
        "            elif act_type == 'leakyrelu':\n",
        "                activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "            self.body.append(activation)\n",
        "\n",
        "        # the last conv\n",
        "        self.body.append(nn.Conv2d(num_feat, num_out_ch * upscale * upscale, 3, 1, 1))\n",
        "        # upsample\n",
        "        self.upsampler = nn.PixelShuffle(upscale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for i in range(0, len(self.body)):\n",
        "            out = self.body[i](out)\n",
        "\n",
        "        out = self.upsampler(out)\n",
        "        # add the nearest upsampled image, so that the network learns the residual\n",
        "        base = F.interpolate(x, scale_factor=self.upscale, mode='nearest')\n",
        "        out += base\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFb1jzXGneCG"
      },
      "source": [
        "### Perceptual Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5ex8DhMneCG"
      },
      "outputs": [],
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    \"\"\"Perceptual loss with commonly used style loss.\n",
        "\n",
        "    Args:\n",
        "        layer_weights (dict): The weight for each layer of vgg feature.\n",
        "            Here is an example: {'conv5_4': 1.}, which means the conv5_4\n",
        "            feature layer (before relu5_4) will be extracted with weight\n",
        "            1.0 in calculating losses.\n",
        "        vgg_type (str): The type of vgg network used as feature extractor.\n",
        "            Default: 'vgg19'.\n",
        "        use_input_norm (bool):  If True, normalize the input image in vgg.\n",
        "            Default: True.\n",
        "        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n",
        "            Default: False.\n",
        "        perceptual_weight (float): If `perceptual_weight > 0`, the perceptual\n",
        "            loss will be calculated and the loss will multiplied by the\n",
        "            weight. Default: 1.0.\n",
        "        style_weight (float): If `style_weight > 0`, the style loss will be\n",
        "            calculated and the loss will multiplied by the weight.\n",
        "            Default: 0.\n",
        "        criterion (str): Criterion used for perceptual loss. Default: 'l1'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 layer_weights,\n",
        "                 vgg_type='vgg19',\n",
        "                 use_input_norm=True,\n",
        "                 range_norm=False,\n",
        "                 perceptual_weight=1.0,\n",
        "                 style_weight=0.,\n",
        "                 criterion='l1'):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.perceptual_weight = perceptual_weight\n",
        "        self.style_weight = style_weight\n",
        "        self.layer_weights = layer_weights\n",
        "        self.vgg = VGGFeatureExtractor(\n",
        "            layer_name_list=list(layer_weights.keys()),\n",
        "            vgg_type=vgg_type,\n",
        "            use_input_norm=use_input_norm,\n",
        "            range_norm=range_norm)\n",
        "\n",
        "        self.criterion_type = criterion\n",
        "        if self.criterion_type == 'l1':\n",
        "            self.criterion = torch.nn.L1Loss()\n",
        "        elif self.criterion_type == 'l2':\n",
        "            self.criterion = torch.nn.MSELoss()\n",
        "        elif self.criterion_type == 'fro':\n",
        "            self.criterion = None\n",
        "        else:\n",
        "            raise NotImplementedError(f'{criterion} criterion has not been supported.')\n",
        "\n",
        "    def forward(self, x, gt):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor with shape (n, c, h, w).\n",
        "            gt (Tensor): Ground-truth tensor with shape (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Forward results.\n",
        "        \"\"\"\n",
        "        # extract vgg features\n",
        "        x_features = self.vgg(x)\n",
        "        gt_features = self.vgg(gt.detach())\n",
        "\n",
        "        # calculate perceptual loss\n",
        "        if self.perceptual_weight > 0:\n",
        "            percep_loss = 0\n",
        "            for k in x_features.keys():\n",
        "                if self.criterion_type == 'fro':\n",
        "                    percep_loss += torch.norm(x_features[k] - gt_features[k], p='fro') * self.layer_weights[k]\n",
        "                else:\n",
        "                    percep_loss += self.criterion(x_features[k], gt_features[k]) * self.layer_weights[k]\n",
        "            percep_loss *= self.perceptual_weight\n",
        "        else:\n",
        "            percep_loss = None\n",
        "\n",
        "        # calculate style loss\n",
        "        if self.style_weight > 0:\n",
        "            style_loss = 0\n",
        "            for k in x_features.keys():\n",
        "                if self.criterion_type == 'fro':\n",
        "                    style_loss += torch.norm(\n",
        "                        self._gram_mat(x_features[k]) - self._gram_mat(gt_features[k]), p='fro') * self.layer_weights[k]\n",
        "                else:\n",
        "                    style_loss += self.criterion(self._gram_mat(x_features[k]), self._gram_mat(\n",
        "                        gt_features[k])) * self.layer_weights[k]\n",
        "            style_loss *= self.style_weight\n",
        "        else:\n",
        "            style_loss = None\n",
        "\n",
        "        return percep_loss, style_loss\n",
        "\n",
        "    def _gram_mat(self, x):\n",
        "        \"\"\"Calculate Gram matrix.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor with shape of (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Gram matrix.\n",
        "        \"\"\"\n",
        "        n, c, h, w = x.size()\n",
        "        features = x.view(n, c, w * h)\n",
        "        features_t = features.transpose(1, 2)\n",
        "        gram = features.bmm(features_t) / (c * h * w)\n",
        "        return gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw6ZbbhQItFh"
      },
      "source": [
        "## Generator Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhB_ozEKItFh"
      },
      "source": [
        "### Meta-sr module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utG9DacAItFh"
      },
      "outputs": [],
      "source": [
        "def repeat(x: Tensor, upscale_factor: int) -> Tensor:\n",
        "    batch_size, channels, height, width = x.size()\n",
        "    out = x.view(batch_size, channels, height, 1, width, 1)\n",
        "\n",
        "    upscale_factor = math.ceil(upscale_factor)\n",
        "    out = torch.cat([out] * upscale_factor, 3)\n",
        "    out = torch.cat([out] * upscale_factor, 5).permute(0, 3, 5, 1, 2, 4)\n",
        "\n",
        "    out = out.contiguous().view(-1, channels, height, width)\n",
        "\n",
        "    return out\n",
        "\n",
        "def weight_prediction_matrix_from_lr(lr_image_height: int, lr_image_width: int, upscale_factor: float):\n",
        "    \"\"\"Weight prediction matrix from LR to SR\"\"\"\n",
        "    sr_image_height, sr_image_width = int(upscale_factor * lr_image_height), int(upscale_factor * lr_image_width)\n",
        "\n",
        "    # Calculate the input matrix by calculating the offset\n",
        "    upscale_factor_int = int(math.ceil(upscale_factor))\n",
        "\n",
        "    height_offset = torch.ones(lr_image_height, upscale_factor_int, 1)\n",
        "    width_offset = torch.ones(1, lr_image_width, upscale_factor_int)\n",
        "\n",
        "    height_mask = torch.zeros(lr_image_height, upscale_factor_int, 1)\n",
        "    width_mask = torch.zeros(1, lr_image_width, upscale_factor_int)\n",
        "\n",
        "    upscale_factor_matrix = torch.zeros(1, 1)\n",
        "    upscale_factor_matrix[0, 0] = 1.0 / upscale_factor\n",
        "    # (lr_image_width * lr_image_height * up_scale_factor ** 2, 1)\n",
        "    upscale_factor_matrix = torch.cat([upscale_factor_matrix] * (lr_image_height * lr_image_width * (upscale_factor_int ** 2)), 0)\n",
        "\n",
        "    # Calculate projected coordinate offsets\n",
        "    height_project_coord_offset = torch.arange(0, sr_image_height, 1).float().mul(1.0 / upscale_factor)\n",
        "    width_project_coord_offset = torch.arange(0, sr_image_width, 1).float().mul(1.0 / upscale_factor)\n",
        "\n",
        "    floor_height_project_coord_offset = torch.floor(height_project_coord_offset)\n",
        "    floor_width_project_coord_offset = torch.floor(width_project_coord_offset)\n",
        "\n",
        "    height_coord_offset = height_project_coord_offset - floor_height_project_coord_offset\n",
        "    width_coord_offset = width_project_coord_offset - floor_width_project_coord_offset\n",
        "\n",
        "    floor_height_project_coord_offset = floor_height_project_coord_offset.int()\n",
        "    floor_width_project_coord_offset = floor_width_project_coord_offset.int()\n",
        "\n",
        "    # Calculate the LR coordinate offset\n",
        "    flag = 0\n",
        "    number = 0\n",
        "    for i in range(sr_image_height):\n",
        "        if floor_height_project_coord_offset[i] == number:\n",
        "            height_offset[floor_height_project_coord_offset[i], flag, 0] = height_coord_offset[i]\n",
        "            height_mask[floor_height_project_coord_offset[i], flag, 0] = 1\n",
        "            flag += 1\n",
        "        else:\n",
        "            height_offset[floor_height_project_coord_offset[i], 0, 0] = height_coord_offset[i]\n",
        "            height_mask[floor_height_project_coord_offset[i], 0, 0] = 1\n",
        "            number += 1\n",
        "            flag = 1\n",
        "\n",
        "    flag = 0\n",
        "    number = 0\n",
        "    for i in range(sr_image_width):\n",
        "        if floor_width_project_coord_offset[i] == number:\n",
        "            width_offset[0, floor_width_project_coord_offset[i], flag] = width_coord_offset[i]\n",
        "            width_mask[0, floor_width_project_coord_offset[i], flag] = 1\n",
        "            flag += 1\n",
        "        else:\n",
        "            width_offset[0, floor_width_project_coord_offset[i], 0] = width_coord_offset[i]\n",
        "            width_mask[0, floor_width_project_coord_offset[i], 0] = 1\n",
        "            number += 1\n",
        "            flag = 1\n",
        "\n",
        "    height_coord_offset = torch.cat([height_offset] * (upscale_factor_int * lr_image_width), 2).view(-1, upscale_factor_int * lr_image_width, 1)\n",
        "    width_coord_offset = torch.cat([width_offset] * (upscale_factor_int * lr_image_height), 0).view(-1, upscale_factor_int * lr_image_width, 1)\n",
        "\n",
        "    height_mask = torch.cat([height_mask] * (upscale_factor_int * lr_image_width), 2).view(-1, upscale_factor_int * lr_image_width, 1)\n",
        "    width_mask = torch.cat([width_mask] * (upscale_factor_int * lr_image_height), 0).view(-1, upscale_factor_int * lr_image_width, 1)\n",
        "\n",
        "    pos_matrix = torch.cat((height_coord_offset, width_coord_offset), 2)\n",
        "    mask_matrix = torch.sum(torch.cat((height_mask, width_mask), 2), 2).view(upscale_factor_int * lr_image_height, upscale_factor_int * lr_image_width)\n",
        "    mask_matrix = mask_matrix.eq(2)\n",
        "    pos_matrix = pos_matrix.contiguous().view(1, -1, 2)\n",
        "\n",
        "    pos_matrix = torch.cat((upscale_factor_matrix.view(1, -1, 1), pos_matrix), 2)\n",
        "\n",
        "    return pos_matrix, mask_matrix\n",
        "\n",
        "class _PosToWeight(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super(_PosToWeight, self).__init__()\n",
        "        self.pos_to_weight = nn.Sequential(\n",
        "            nn.Linear(3, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 3 * 3 * in_channels * out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        out = self.pos_to_weight(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar0xqRmFItFi"
      },
      "source": [
        "### RRDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bPEbhBGItFi"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
        "    \"\"\"Initialize network weights.\n",
        "\n",
        "    Args:\n",
        "        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "        scale (float): Scale initialized weights, especially for residual\n",
        "            blocks. Default: 1.\n",
        "        bias_fill (float): The value to fill bias. Default: 0\n",
        "        kwargs (dict): Other arguments for initialization function.\n",
        "    \"\"\"\n",
        "    if not isinstance(module_list, list):\n",
        "        module_list = [module_list]\n",
        "    for module in module_list:\n",
        "        for m in module.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, **kwargs)\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "\n",
        "class ResidualDenseBlock(nn.Module):\n",
        "    \"\"\"Residual Dense Block.\n",
        "\n",
        "    Used in RRDB block in ESRGAN.\n",
        "\n",
        "    Args:\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "        num_grow_ch (int): Channels for each growth.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_feat=64, num_grow_ch=32):\n",
        "        super(ResidualDenseBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_feat, num_grow_ch, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(num_feat + num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(num_feat + 2 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv4 = nn.Conv2d(num_feat + 3 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
        "        self.conv5 = nn.Conv2d(num_feat + 4 * num_grow_ch, num_feat, 3, 1, 1)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        # initialization\n",
        "        default_init_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.lrelu(self.conv1(x))\n",
        "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
        "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
        "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        # Empirically, we use 0.2 to scale the residual for better performance\n",
        "        return x5 * 0.2 + x\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    \"\"\"Residual in Residual Dense Block.\n",
        "\n",
        "    Used in RRDB-Net in ESRGAN.\n",
        "\n",
        "    Args:\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "        num_grow_ch (int): Channels for each growth.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_feat, num_grow_ch=32):\n",
        "        super(RRDB, self).__init__()\n",
        "        self.rdb1 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "        self.rdb2 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "        self.rdb3 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.rdb1(x)\n",
        "        out = self.rdb2(out)\n",
        "        out = self.rdb3(out)\n",
        "        # Empirically, we use 0.2 to scale the residual for better performance\n",
        "        return out * 0.2 + x\n",
        "\n",
        "def make_layer(basic_block, num_basic_block, **kwarg):\n",
        "    \"\"\"Make layers by stacking the same blocks.\n",
        "\n",
        "    Args:\n",
        "        basic_block (nn.module): nn.module class for basic block.\n",
        "        num_basic_block (int): number of blocks.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for _ in range(num_basic_block):\n",
        "        layers.append(basic_block(**kwarg))\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cduvkmK9ItFi"
      },
      "source": [
        "### Main model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNGJszafM-gg"
      },
      "outputs": [],
      "source": [
        "class RRDBNet(nn.Module):\n",
        "    \"\"\"Networks consisting of Residual in Residual Dense Block, which is used\n",
        "    in ESRGAN.\n",
        "\n",
        "    ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks.\n",
        "\n",
        "    We extend ESRGAN for scale x2 and scale x1.\n",
        "    Note: This is one option for scale 1, scale 2 in RRDBNet.\n",
        "    We first employ the pixel-unshuffle (an inverse operation of pixelshuffle to reduce the spatial size\n",
        "    and enlarge the channel size before feeding inputs into the main ESRGAN architecture.\n",
        "\n",
        "    Args:\n",
        "        num_in_ch (int): Channel number of inputs.\n",
        "        num_out_ch (int): Channel number of outputs.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "            Default: 64\n",
        "        num_block (int): Block number in the trunk network. Defaults: 23\n",
        "        num_grow_ch (int): Channels for each growth. Default: 32.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_in_ch=3, num_feat=64, num_block=23, num_grow_ch=32):\n",
        "        super(RRDBNet, self).__init__()\n",
        "\n",
        "        self.conv_last1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_last2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_last3 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
        "        nn.init.kaiming_normal_(self.conv_last1.weight, a=0, mode='fan_in')\n",
        "        nn.init.constant_(self.conv_last1.bias, 0)\n",
        "        nn.init.kaiming_normal_(self.conv_last2.weight, a=0, mode='fan_in')\n",
        "        nn.init.constant_(self.conv_last2.bias, 0)\n",
        "        nn.init.kaiming_normal_(self.conv_last3.weight, a=0, mode='fan_in')\n",
        "        nn.init.constant_(self.conv_last3.bias, 0)\n",
        "\n",
        "        self.conv_first = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)\n",
        "        self.body = make_layer(RRDB, num_block, num_feat=num_feat, num_grow_ch=num_grow_ch)\n",
        "        self.conv_body = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "\n",
        "        self.pos_to_weight = _PosToWeight(64, 3)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    # Meta-sr\n",
        "    def meta_sr(self, out, pos_matrix, upscale_factor):\n",
        "        upscale_factor_int = int(math.ceil(upscale_factor))\n",
        "\n",
        "        pos_matrix = pos_matrix.view(pos_matrix.size(1), -1)\n",
        "        local_weight = self.pos_to_weight(pos_matrix)\n",
        "\n",
        "        repeat_out = repeat(out, upscale_factor_int)\n",
        "        cols = nn.functional.unfold(repeat_out, (3, 3), padding=1)\n",
        "\n",
        "        cols = cols.contiguous().view(cols.size(0) // (upscale_factor_int ** 2),\n",
        "                                      upscale_factor_int ** 2,\n",
        "                                      cols.size(1),\n",
        "                                      cols.size(2), 1).permute(0, 1, 3, 4, 2).contiguous()\n",
        "\n",
        "        local_weight = local_weight.contiguous().view(out.size(2),\n",
        "                                                      upscale_factor_int,\n",
        "                                                      out.size(3),\n",
        "                                                      upscale_factor_int,\n",
        "                                                      -1,\n",
        "                                                      3).permute(1, 3, 0, 2, 4, 5).contiguous()\n",
        "        local_weight = local_weight.contiguous().view(upscale_factor_int ** 2,\n",
        "                                                      out.size(2) * out.size(3),\n",
        "                                                      -1,\n",
        "                                                      3)\n",
        "\n",
        "        outs = torch.matmul(cols, local_weight).permute(0, 1, 4, 2, 3)\n",
        "        outs = outs.contiguous().view(out.size(0),\n",
        "                                      upscale_factor_int,\n",
        "                                      upscale_factor_int,\n",
        "                                      3,\n",
        "                                      out.size(2),\n",
        "                                      out.size(3)).permute(0, 3, 4, 1, 5, 2)\n",
        "        outs = outs.contiguous().view(out.size(0),\n",
        "                                      3,\n",
        "                                      upscale_factor_int * out.size(2),\n",
        "                                      upscale_factor_int * out.size(3))\n",
        "\n",
        "        return outs\n",
        "\n",
        "    def forward(self, x, pos_matrix, upscale_factor):\n",
        "        feat = x\n",
        "        feat = self.conv_first(feat)\n",
        "        body_feat = self.conv_body(self.body(feat))\n",
        "        feat = feat + body_feat\n",
        "\n",
        "        # meta-sr\n",
        "        out = self.meta_sr(feat, pos_matrix, upscale_factor)\n",
        "        residual = self.lrelu(self.conv_last1(out))\n",
        "        residual = self.lrelu(self.conv_last2(residual))\n",
        "        residual = self.conv_last3(residual)\n",
        "\n",
        "        out = out + residual\n",
        "        out = out.clamp_(0.0, 1.0)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njJA8rUMItFi"
      },
      "source": [
        "## PSNR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YydniZ5FItFi"
      },
      "outputs": [],
      "source": [
        "#Return psnr value between torch.tensor images\n",
        "def psnr_between_rgb(img1,img2):\n",
        "    if len(img1.shape) == 4:\n",
        "        img1 = img1.squeeze(0)\n",
        "        img2 = img2.squeeze(0)\n",
        "    y1 = 16. + (64.738 * img1[0, :, :] + 129.057 * img1[1, :, :] + 25.064 * img1[2, :, :]) / 256.\n",
        "    y2 = 16. + (64.738 * img2[0, :, :] + 129.057 * img2[1, :, :] + 25.064 * img2[2, :, :]) / 256.\n",
        "    psnr = (10. * torch.log10(1. / torch.mean((y1 - y2) ** 2)))\n",
        "    return psnr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gCt3VpMItFi"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2CEf9cuneCH"
      },
      "outputs": [],
      "source": [
        "def get_data(data, upscale_factor, lr_image_size, device):\n",
        "    gt = transforms.RandomCrop([int(upscale_factor * lr_image_size),\n",
        "                                    int(upscale_factor * lr_image_size)])(data)\n",
        "    lr = transforms.Resize([lr_image_size, lr_image_size], interpolation=IMode.BICUBIC)(gt)\n",
        "\n",
        "    gt = gt.to(device, non_blocking=True)\n",
        "    lr = lr.to(device, non_blocking=True)\n",
        "\n",
        "    return gt, lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTFf1wbqItFj"
      },
      "source": [
        "## Without Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqZVImbWItFj"
      },
      "source": [
        "### Hyper Parameters & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPfLTqm2ItFj",
        "outputId": "9ae04bde-0fb8-4961-e00c-ade8e02872c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model loaded: \n"
          ]
        }
      ],
      "source": [
        "##########  MODEL\n",
        "model_root = os.path.join(drive_root,\"model\")\n",
        "\n",
        "######### PARAMETER\n",
        "model_load = None\n",
        "\n",
        "learning_rate = 1e-4\n",
        "training_epochs = 15\n",
        "\n",
        "batchsize = 4\n",
        "num_workers = 0\n",
        "userseed = 123\n",
        "\n",
        "lr_image_size = 60\n",
        "min_scale, max_scale = 1.95, 4.05\n",
        "####################################################################################\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(userseed)\n",
        "\n",
        "class srDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        super(srDataset, self).__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.files = os.listdir(root_dir)\n",
        "        #del self.files[100:]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.files[idx])\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        image = np.array(image).transpose((2,0,1)).astype(np.float32)/255.\n",
        "\n",
        "        return image\n",
        "\n",
        "train_dataset = srDataset(os.path.join(drive_root,'dataset/train'), transform = transforms.Compose([\n",
        "    transforms.RandomCrop((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5)\n",
        "]))\n",
        "\n",
        "train_dataloader = DataLoader(dataset = train_dataset,\n",
        "                                batch_size = batchsize,\n",
        "                                shuffle = True,\n",
        "                                num_workers = num_workers,\n",
        "                                pin_memory=True,\n",
        "                                drop_last = True,)\n",
        "\n",
        "valid_dataset = srDataset(os.path.join(drive_root,'dataset/valid'), transform = transforms.Compose([\n",
        "    transforms.RandomCrop((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5)\n",
        "]))\n",
        "\n",
        "valid_dataloader = DataLoader(dataset = valid_dataset,\n",
        "                                batch_size = batchsize,\n",
        "                                shuffle = True,\n",
        "                                num_workers = num_workers,\n",
        "                                pin_memory=True,\n",
        "                                drop_last = True,)\n",
        "\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(userseed)\n",
        "\n",
        "#models\n",
        "model = RRDBNet().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#loading model state\n",
        "if model_load is not None:\n",
        "    model.load_state_dict(torch.load(os.path.join(model_root, model_load),weights_only=True))\n",
        "    print(\"model loaded: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "fGn0f1KyneCH",
        "outputId": "ad27528e-c605-4e1c-9713-78c49b37245b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-323fef29bf68>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights = torch.load(os.path.join(drive_root, \"model\",\"RRDB_ESRGAN_x4.pth\"))\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/sryeo/model/RRDB_ESRGAN_x4.pth'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-323fef29bf68>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### load ESRGAN pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"RRDB_ESRGAN_x4.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"conv_first.weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"conv_first.weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"conv_first.bias\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"conv_first.bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trunk_conv.weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"conv_body.weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trunk_conv.bias\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"conv_body.bias\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/sryeo/model/RRDB_ESRGAN_x4.pth'"
          ]
        }
      ],
      "source": [
        "### load ESRGAN pretrained model\n",
        "### Run when not loading other models\n",
        "\n",
        "weights = torch.load(os.path.join(drive_root, \"pretrained\", \"RRDB_ESRGAN_x4.pth\"))\n",
        "\n",
        "dic = {\"conv_first.weight\": \"conv_first.weight\", \"conv_first.bias\": \"conv_first.bias\", \"trunk_conv.weight\": \"conv_body.weight\", \"trunk_conv.bias\": \"conv_body.bias\"}\n",
        "for i in range(23):\n",
        "    for j in range(3):\n",
        "        for k in range(5):\n",
        "            dic[f\"RRDB_trunk.{i}.RDB{j+1}.conv{k+1}.weight\"] = f\"body.{i}.rdb{j+1}.conv{k+1}.weight\"\n",
        "            dic[f\"RRDB_trunk.{i}.RDB{j+1}.conv{k+1}.bias\"] = f\"body.{i}.rdb{j+1}.conv{k+1}.bias\"\n",
        "\n",
        "layer_weights = {v: weights[k]  for k, v in dic.items()}\n",
        "\n",
        "model = RRDBNet().to(device)\n",
        "model.load_state_dict(layer_weights, strict=False)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEb-vnMDItFj"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_QptzyVItFj"
      },
      "outputs": [],
      "source": [
        "# 학습 시작 시간\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    model.train()\n",
        "    avg_cost=0\n",
        "    epoch_start_time = time.time()  # 에포크 시작 시간\n",
        "\n",
        "    # Freeze all model parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze the parameters of the new convolutional layers\n",
        "    for param in model.conv_last1.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.conv_last2.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.conv_last3.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for  batch_idx, data in enumerate(tqdm(train_dataloader)):\n",
        "        upscale_factor = max(random.uniform(min_scale, max_scale),random.uniform(min_scale, max_scale))\n",
        "\n",
        "\n",
        "        # According to the requirement in the paper, the lr image size should be 50\n",
        "        gt, lr = get_data(data, upscale_factor, lr_image_size, device)\n",
        "\n",
        "        # gt = transforms.RandomCrop([int(upscale_factor * lr_image_size),\n",
        "        #                             int(upscale_factor * lr_image_size)])(data)\n",
        "        # lr = transforms.Resize([lr_image_size, lr_image_size], interpolation=IMode.BICUBIC)(gt)\n",
        "\n",
        "        # gt = gt.to(device, non_blocking=True)\n",
        "        # lr = lr.to(device, non_blocking=True)\n",
        "\n",
        "        batch_size, channels, lr_height, lr_width = lr.size()\n",
        "        _, _, gt_height, gt_width = gt.size()\n",
        "\n",
        "        # Get the position matrix, mask\n",
        "        pos_matrix, mask_matrix = weight_prediction_matrix_from_lr(lr_height, lr_width, upscale_factor)\n",
        "        pos_matrix = pos_matrix.to(device, non_blocking=True)\n",
        "        mask_matrix = mask_matrix.to(device, non_blocking=True)\n",
        "\n",
        "        # Initialize the generator gradient\n",
        "        model.zero_grad(set_to_none=True)\n",
        "\n",
        "        sr = model(lr, pos_matrix, upscale_factor)\n",
        "        sr = torch.masked_select(sr, mask_matrix)\n",
        "        sr = sr.contiguous().view(batch_size, channels, gt_height, gt_width)\n",
        "        loss = criterion(sr, gt)\n",
        "\n",
        "        # loss\n",
        "        avg_cost += loss\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # update generator weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # 배치 단위로 손실 출력 (1000 배치마다 출력)\n",
        "        if(batch_idx + 1) % 1000 == 0:\n",
        "            batch_time = time.time() - epoch_start_time\n",
        "            print(f\"[Batch {batch_idx+1}/{len(train_dataloader)}] [L1 Loss: {loss.item():.4f}] {batch_time:.1f}s\")\n",
        "\n",
        "\n",
        "    # 에포크 완료 후 평균 손실과 경과 시간 출력\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    avg_cost /= len(train_dataloader)  # 에포크 단위 평균 손실 계산\n",
        "    print(f\"[Epoch {epoch + 1}] cost = {avg_cost:.9f}, Time = {epoch_time:.1f}s\")\n",
        "\n",
        "    # Validation set\n",
        "    model.eval()\n",
        "    sum_psnr = 0\n",
        "\n",
        "    for data in tqdm(valid_dataloader):\n",
        "        upscale_factor = max(random.uniform(min_scale, max_scale),random.uniform(min_scale, max_scale))\n",
        "\n",
        "        # According to the requirement in the paper, the lr image size should be 50\n",
        "        gt = transforms.RandomCrop([int(upscale_factor * lr_image_size),\n",
        "                                    int(upscale_factor * lr_image_size)])(data)\n",
        "        lr = transforms.Resize([lr_image_size, lr_image_size], interpolation=IMode.BICUBIC)(gt)\n",
        "\n",
        "        gt = gt.to(device, non_blocking=True)\n",
        "        lr = lr.to(device, non_blocking=True)\n",
        "\n",
        "        # Get the position matrix, mask\n",
        "        batch_size, channels, lr_height, lr_width = lr.size()\n",
        "        _, _, gt_height, gt_width = gt.size()\n",
        "        pos_matrix, mask_matrix = weight_prediction_matrix_from_lr(lr_height, lr_width, upscale_factor)\n",
        "        pos_matrix = pos_matrix.to(device, non_blocking=True)\n",
        "        mask_matrix = mask_matrix.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sr = model(lr, pos_matrix, upscale_factor)\n",
        "            sr = torch.masked_select(sr, mask_matrix)\n",
        "            sr = sr.contiguous().view(batch_size, channels, gt_height, gt_width)\n",
        "\n",
        "            sum_psnr += psnr_between_rgb(gt, sr)\n",
        "\n",
        "    print('PSNR: {:.2f}'.format(sum_psnr/len(valid_dataloader)))\n",
        "\n",
        "    # 5 epoch 마다 저장\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), os.path.join(model_root,f\"g_model_{epoch+1}.pth\"))\n",
        "        print(\"saved\")\n",
        "\n",
        "print(\"done\")\n",
        "torch.save(model.state_dict(), os.path.join(model_root,\"g_model.pth\"))\n",
        "print(\"saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk6cNh9iItFk"
      },
      "source": [
        "## With Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtJIzvcBneCI"
      },
      "outputs": [],
      "source": [
        "def zero_pad_to(images, sz):\n",
        "    _, _, h, w = images.shape\n",
        "    target_size = int(sz)\n",
        "\n",
        "    pad_h = (target_size - h) // 2\n",
        "    pad_w = (target_size - w) // 2\n",
        "\n",
        "    padding = (pad_w, target_size - w - pad_w, pad_h, target_size - h - pad_h)\n",
        "\n",
        "    # Apply padding to each image in the batch\n",
        "    padded_images = nn.functional.pad(images, padding, mode='constant', value=0)\n",
        "    return padded_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxcwHiGcuxY"
      },
      "source": [
        "### Hype Parameters & Preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnzDTdvRItFk",
        "outputId": "918aa14c-909c-4e44-d905-59342f5907b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:10<00:00, 52.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "g_model loaded: /content/drive/MyDrive/sryeo/model/g_model_50.pth\n",
            "d_model loaded: /content/drive/MyDrive/sryeo/model/d_model_50.pth\n"
          ]
        }
      ],
      "source": [
        "##########  MODEL\n",
        "# model_parameters_path = os.path.join(drive_root,\"model/model.pth\")\n",
        "# d_model_parameters_path = os.path.join(drive_root,\"model/d_model.pth\")\n",
        "\n",
        "model_root = os.path.join(drive_root,\"model\")\n",
        "\n",
        "g_model_load = \"g_model.pth\"\n",
        "d_model_load = None\n",
        "\n",
        "######### PARAMETER\n",
        "# g_from_scratch = False\n",
        "# d_from_scratch = True\n",
        "learning_rate = 7e-6\n",
        "lr2 = 4e-6\n",
        "lr3 = 7e-6\n",
        "\n",
        "start_epoch = 0\n",
        "training_epochs = 100\n",
        "batchsize = 4\n",
        "num_workers = 0\n",
        "userseed = 123\n",
        "\n",
        "lr_image_size = 60\n",
        "pad_image_size = 256\n",
        "\n",
        "min_scale=1.95\n",
        "max_scale=4.05\n",
        "# ema_decay = 0.999\n",
        "####################################################################################\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(userseed)\n",
        "\n",
        "class srDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        super(srDataset, self).__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.files = os.listdir(root_dir)\n",
        "        # del self.files[100:]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.files[idx])\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        image = np.array(image).transpose((2,0,1)).astype(np.float32)/255.\n",
        "\n",
        "        return image\n",
        "\n",
        "train_dataset = srDataset(os.path.join(drive_root,'dataset/train'), transform = transforms.Compose([\n",
        "    transforms.RandomCrop((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5)\n",
        "]))\n",
        "\n",
        "train_dataloader = DataLoader(dataset = train_dataset,\n",
        "                                batch_size = batchsize,\n",
        "                                shuffle = True,\n",
        "                                num_workers = num_workers,\n",
        "                                pin_memory=True,\n",
        "                                drop_last = True,)\n",
        "\n",
        "valid_dataset = srDataset(os.path.join(drive_root,'dataset/valid'), transform = transforms.Compose([\n",
        "    transforms.RandomCrop((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5)\n",
        "]))\n",
        "\n",
        "valid_dataloader = DataLoader(dataset = valid_dataset,\n",
        "                                batch_size = batchsize,\n",
        "                                shuffle = True,\n",
        "                                num_workers = num_workers,\n",
        "                                pin_memory=True,\n",
        "                                drop_last = True,)\n",
        "\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(userseed)\n",
        "\n",
        "#models\n",
        "g_model = RRDBNet().to(device)\n",
        "d_model = VGGStyleDiscriminator(3, 64, 256).to(device)\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "# g_optimizer = torch.optim.Adam(\n",
        "#     list(g_model.conv_last1.parameters()) + list(g_model.conv_last2.parameters()) + list(g_model.conv_last3.parameters()),\n",
        "#     lr=learning_rate\n",
        "# )\n",
        "# Define the parameters explicitly for Group 1\n",
        "# Define the parameter groups explicitly by layer\n",
        "group1_params = list(g_model.conv_last1.parameters()) + \\\n",
        "                list(g_model.conv_last2.parameters()) + \\\n",
        "                list(g_model.conv_last3.parameters())\n",
        "\n",
        "# Use named_parameters to collect all model parameters\n",
        "group1_names = [id(p) for p in group1_params]  # Use `id` to track parameter memory addresses\n",
        "group2_params = [p for p in g_model.parameters() if id(p) not in group1_names]\n",
        "\n",
        "# Define the optimizer\n",
        "g_optimizer = torch.optim.Adam(\n",
        "    [\n",
        "        # Group 1: Specific layers with learning rate `learning_rate`\n",
        "        {'params': group1_params, 'lr': learning_rate},\n",
        "\n",
        "        # Group 2: Remaining layers with learning rate `lr2`\n",
        "        {'params': group2_params, 'lr': lr2}\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "###\n",
        "d_optimizer = torch.optim.Adam(d_model.parameters(), lr = lr3)\n",
        "\n",
        "cri_perceptual = PerceptualLoss({'conv5_4': 1}, 'vgg19', True, False, 1.0, 0, 'l1').to(device)\n",
        "cri_gan = GANLoss('wgan_softplus', 1, 0, 5e-3).to(device)\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#loading model state\n",
        "# if g_from_scratch is False:\n",
        "if g_model_load is not None:\n",
        "    g_model.load_state_dict(torch.load(os.path.join(model_root, g_model_load),weights_only=True))\n",
        "    print(\"g_model loaded:\", os.path.join(model_root, g_model_load))\n",
        "\n",
        "# if d_from_scratch is False:\n",
        "if d_model_load is not None:\n",
        "    d_model.load_state_dict(torch.load(os.path.join(model_root, d_model_load), weights_only=True))\n",
        "    print(\"d_model loaded:\", os.path.join(model_root, d_model_load))\n",
        "else:\n",
        "    print(\"d_model automatically loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6CwYAKKcyxV"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XLBN2jWneCI"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, training_epochs):\n",
        "    g_model.train()\n",
        "    d_model.train()\n",
        "\n",
        "\n",
        "\n",
        "    avg_l_g = 0\n",
        "    avg_l_d = 0\n",
        "\n",
        "    for data in tqdm(train_dataloader):\n",
        "        ### data processing\n",
        "        #upscale_factor = random.choice(upscale_factors)\n",
        "        upscale_factor = max (random.uniform(min_scale, max_scale), random.uniform(min_scale, max_scale))\n",
        "\n",
        "        gt, lr = get_data(data, upscale_factor, lr_image_size, device)\n",
        "\n",
        "        # gt = transforms.RandomCrop([int(upscale_factor * lr_image_size),\n",
        "        #                             int(upscale_factor * lr_image_size)])(data)\n",
        "        # lr = transforms.Resize([lr_image_size, lr_image_size], interpolation=IMode.BICUBIC)(gt)\n",
        "        # gt = gt.to(device, non_blocking=True)\n",
        "        # lr = lr.to(device, non_blocking=True)\n",
        "\n",
        "        batch_size, channels, lr_height, lr_width = lr.size()\n",
        "        _, _, gt_height, gt_width = gt.size()\n",
        "\n",
        "        pos_matrix, mask_matrix = weight_prediction_matrix_from_lr(lr_height, lr_width, upscale_factor)\n",
        "        pos_matrix = pos_matrix.to(device, non_blocking=True)\n",
        "        mask_matrix = mask_matrix.to(device, non_blocking=True)\n",
        "\n",
        "        for p in d_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        g_optimizer.zero_grad()\n",
        "        sr = g_model(lr, pos_matrix, upscale_factor)\n",
        "        sr = torch.masked_select(sr, mask_matrix)\n",
        "        sr = sr.contiguous().view(batch_size, channels, gt_height, gt_width)\n",
        "\n",
        "        sr = zero_pad_to(sr, pad_image_size)\n",
        "        gt = zero_pad_to(gt, pad_image_size)\n",
        "\n",
        "        l_g_total = 0\n",
        "        # loss_dict = OrderedDict()\n",
        "\n",
        "        # perceptual loss\n",
        "        l_g_percep, l_g_style = cri_perceptual(sr, gt)\n",
        "        if l_g_percep is not None:\n",
        "            l_g_total += l_g_percep\n",
        "            # loss_dict['l_g_percep'] = l_g_percep\n",
        "        if l_g_style is not None:\n",
        "            l_g_total += l_g_style\n",
        "            # loss_dict['l_g_style'] = l_g_style\n",
        "\n",
        "        # gan loss (relativistic gan)\n",
        "        real_d_pred = d_model(gt).detach()\n",
        "        fake_g_pred = d_model(sr)\n",
        "        l_g_real = cri_gan(real_d_pred - torch.mean(fake_g_pred), False, is_disc=False)\n",
        "        l_g_fake = cri_gan(fake_g_pred - torch.mean(real_d_pred), True, is_disc=False)\n",
        "        l_g_gan = (l_g_real + l_g_fake) / 2\n",
        "\n",
        "        l_g_total += l_g_gan\n",
        "        # loss_dict['l_g_gan'] = l_g_gan\n",
        "        avg_l_g += l_g_gan\n",
        "\n",
        "        l_g_total.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # optimize net_d\n",
        "        for p in d_model.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        d_optimizer.zero_grad()\n",
        "        # gan loss (relativistic gan)\n",
        "\n",
        "        # In order to avoid the error in distributed training:\n",
        "        # \"Error detected in CudnnBatchNormBackward: RuntimeError: one of\n",
        "        # the variables needed for gradient computation has been modified by\n",
        "        # an inplace operation\",\n",
        "        # we separate the backwards for real and fake, and also detach the\n",
        "        # tensor for calculating mean.\n",
        "\n",
        "        # real\n",
        "        fake_d_pred = d_model(sr).detach()\n",
        "        real_d_pred = d_model(gt)\n",
        "        l_d_real = cri_gan(real_d_pred - torch.mean(fake_d_pred), True, is_disc=True) * 0.5\n",
        "        l_d_real.backward()\n",
        "        # fake\n",
        "        fake_d_pred = d_model(sr.detach())\n",
        "        l_d_fake = cri_gan(fake_d_pred - torch.mean(real_d_pred.detach()), False, is_disc=True) * 0.5\n",
        "        l_d_fake.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        avg_l_d += l_d_real + l_d_fake\n",
        "\n",
        "    avg_l_g /= len(train_dataloader)\n",
        "    avg_l_d /= len(train_dataloader)\n",
        "    print('[Epoch: {:>4}] avg_gen_loss = {:>.9} avg_dis_loss = {:>.9}'.format(epoch + 1, avg_l_g, avg_l_d))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "    #if True:\n",
        "        torch.save(g_model.state_dict(), os.path.join(model_root, f\"g_model_{epoch+1}.pth\"))\n",
        "        torch.save(d_model.state_dict(), os.path.join(model_root, f\"d_model_{epoch+1}.pth\"))\n",
        "        print(\"saved\")\n",
        "        ### eval\n",
        "        g_model.eval()\n",
        "        d_model.eval()\n",
        "        sum_psnr = 0\n",
        "        f_score = 0\n",
        "        t_score = 0\n",
        "\n",
        "        for data in tqdm(valid_dataloader):\n",
        "            upscale_factor = random.choice(upscale_factors)\n",
        "\n",
        "            gt, lr = get_data(data, upscale_factor, lr_image_size, device)\n",
        "\n",
        "            batch_size, channels, lr_height, lr_width = lr.size()\n",
        "            _, _, gt_height, gt_width = gt.size()\n",
        "\n",
        "            pos_matrix, mask_matrix = weight_prediction_matrix_from_lr(lr_height, lr_width, upscale_factor)\n",
        "            pos_matrix = pos_matrix.to(device, non_blocking=True)\n",
        "            mask_matrix = mask_matrix.to(device, non_blocking=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sr = g_model(lr, pos_matrix, upscale_factor)\n",
        "                sr = torch.masked_select(sr, mask_matrix)\n",
        "                sr = sr.contiguous().view(batch_size, channels, gt_height, gt_width)\n",
        "                sum_psnr += psnr_between_rgb(gt, sr)\n",
        "\n",
        "                sr = zero_pad_to(sr, pad_image_size)\n",
        "                gt = zero_pad_to(gt, pad_image_size)\n",
        "\n",
        "                f_score += (d_model(sr)<0.5).sum().item()/batch_size\n",
        "                t_score += (d_model(gt)>0.5).sum().item()/batch_size\n",
        "\n",
        "        ###\n",
        "\n",
        "        print('PSNR: {:.2f}, D_Fake_Acc: {:.2f}%, D_Truth_ACC: {:.2f}%'.format(sum_psnr/len(valid_dataloader), f_score*100.0/len(valid_dataloader), t_score*100.0/len(valid_dataloader)))\n",
        "\n",
        "\n",
        "print(\"done\")\n",
        "torch.save(g_model.state_dict(), os.path.join(model_root, \"g_model.pth\"))\n",
        "torch.save(d_model.state_dict(), os.path.join(model_root, \"d_model.pth\"))\n",
        "print(\"saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6RmhLZIItFk"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7W3t5G_-XmS",
        "outputId": "75248ad7-d063-4e72-f986-522185b9193e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "skipped\n",
            "Average PSNR over all images: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# For large image testing\n",
        "# Divides the image and puts them back together after SR\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#################\n",
        "upscale_factors = [2.22]\n",
        "#################\n",
        "\n",
        "test_root = os.path.join(drive_root, 'dataset/test')\n",
        "files = os.listdir(os.path.join(test_root, 'origin'))\n",
        "\n",
        "# model = Generator().to(device)\n",
        "# model.load_state_dict(torch.load(os.path.join(drive_root, \"model/dis_train1/model_50.pth\"), weights_only=True))\n",
        "g_model.eval()\n",
        "\n",
        "avg = 0\n",
        "\n",
        "i = 0\n",
        "for upscale_factor in upscale_factors:\n",
        "  original = upscale_factor\n",
        "  upscale_factor = round(upscale_factor, 1)\n",
        "  for file in files:\n",
        "      # Open the high-resolution image\n",
        "      Error = False\n",
        "      with Image.open(os.path.join(test_root, 'origin', file)) as hr_image:\n",
        "\n",
        "          hr_width, hr_height = hr_image.size\n",
        "\n",
        "          if hr_width <= 256 or hr_height <= 256:\n",
        "              continue\n",
        "\n",
        "          N_w = 1 if hr_width <= 512 else max(1, math.ceil(hr_width / 512))\n",
        "          N_h = 1 if hr_height <= 512 else max(1, math.ceil(hr_height / 512))\n",
        "\n",
        "          patch_width = hr_width // N_w\n",
        "          patch_height = hr_height // N_h\n",
        "          width = patch_width\n",
        "          height = patch_height\n",
        "\n",
        "          sr_full = np.zeros((hr_height, hr_width, 3), dtype=np.uint8)\n",
        "\n",
        "          total_psnr = 0\n",
        "          count = 0\n",
        "          Error = False\n",
        "\n",
        "          for i in range(N_h):\n",
        "              if Error:\n",
        "                break\n",
        "\n",
        "              for j in range(N_w):\n",
        "                  if Error:\n",
        "                    break\n",
        "                  if i == 0 and j == 0 :\n",
        "                      current_patch_width = patch_width\n",
        "                      current_patch_height = patch_height\n",
        "                  else:\n",
        "                      current_patch_width = width\n",
        "                      current_patch_height = height\n",
        "\n",
        "                  left = j * current_patch_width\n",
        "                  upper = i * current_patch_height\n",
        "                  right = min(hr_width, left + current_patch_width)\n",
        "                  lower = min(hr_height, upper + current_patch_height)\n",
        "\n",
        "                  hr_patch = hr_image.crop((left, upper, right, lower))\n",
        "\n",
        "                  lr_width = int(round(hr_patch.width / upscale_factor))\n",
        "                  lr_height = int(round(hr_patch.height / upscale_factor))\n",
        "\n",
        "                  adjusted_hr_width = int(round(lr_width * upscale_factor))\n",
        "                  adjusted_hr_height = int(round(lr_height * upscale_factor))\n",
        "\n",
        "                  hr_patch = hr_patch.resize((adjusted_hr_width, adjusted_hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "                  lr_patch = hr_patch.resize((lr_width, lr_height), resample=Image.BICUBIC)\n",
        "\n",
        "                  hr = np.array(hr_patch).astype(np.float32).transpose((2, 0, 1)) / 255.\n",
        "                  hr = np.expand_dims(hr, 0)\n",
        "                  lr = np.array(lr_patch).astype(np.float32).transpose((2, 0, 1)) / 255.\n",
        "                  lr = np.expand_dims(lr, 0)\n",
        "\n",
        "                  hr = torch.from_numpy(hr).to(device, non_blocking=True)\n",
        "                  lr = torch.from_numpy(lr).to(device, non_blocking=True)\n",
        "\n",
        "                  batch_size, channels, lr_height_patch, lr_width_patch = lr.size()\n",
        "                  _, _, hr_height_patch, hr_width_patch = hr.size()\n",
        "\n",
        "                  pos_matrix, mask_matrix = weight_prediction_matrix_from_lr(lr_height_patch, lr_width_patch, upscale_factor)\n",
        "                  pos_matrix = pos_matrix.to(device, non_blocking=True)\n",
        "                  mask_matrix = mask_matrix.to(device, non_blocking=True)\n",
        "                  try:\n",
        "                    with torch.no_grad():\n",
        "                        sr = g_model(lr, pos_matrix, upscale_factor)\n",
        "                        sr = torch.masked_select(sr, mask_matrix)\n",
        "\n",
        "                        sr = sr.contiguous().view(batch_size, channels, hr_height_patch, hr_width_patch)\n",
        "\n",
        "\n",
        "                    psnr_patch = psnr_between_rgb(hr, sr)\n",
        "                  except RuntimeError as e:\n",
        "                      Error = True\n",
        "                  if Error:\n",
        "                    break\n",
        "                  total_psnr += psnr_patch\n",
        "                  count += 1\n",
        "\n",
        "                  sr_np = sr.mul(255.0).cpu().numpy()\n",
        "                  sr_np = np.clip(sr_np, 0.0, 255.0).astype(np.uint8)\n",
        "                  sr_np = sr_np.squeeze().transpose((1, 2, 0))\n",
        "\n",
        "                  height, width, channels = sr_np.shape\n",
        "\n",
        "\n",
        "                  sr_full[upper:upper + height, left:left + width, :] = sr_np\n",
        "\n",
        "                  # Clean up GPU memory\n",
        "                  del hr, lr, sr, sr_np, pos_matrix, mask_matrix\n",
        "                  torch.cuda.empty_cache()\n",
        "          if Error:\n",
        "            print(\"skipped\")\n",
        "            continue\n",
        "          i+=1\n",
        "          avg_psnr = total_psnr / count\n",
        "          avg += avg_psnr\n",
        "          print(f'{file}, Average PSNR: {avg_psnr:.3f}')\n",
        "\n",
        "          original_name = os.path.splitext(file)[0]\n",
        "          # Save the full SR image\n",
        "          sr_image_name = f\"sr_{original_name}_factor_{original}_psnr_{avg_psnr:.2f}.png\"\n",
        "          sr_image_full = Image.fromarray(sr_full)\n",
        "          sr_image_full.save(os.path.join(test_root, 'sr', sr_image_name))\n",
        "\n",
        "          # Save the high-resolution (HR) image with updated naming convention\n",
        "          hr_image_name = f\"hr_{original_name}_original.png\"\n",
        "          hr_image.save(os.path.join(test_root, 'hr', hr_image_name))\n",
        "\n",
        "          # Save the low-resolution (LR) image with updated naming convention\n",
        "          lr_image_name = f\"lr_{original_name}_factor_{original}.png\"\n",
        "          lr_image_full = hr_image.resize((int(hr_width / upscale_factor), int(hr_height / upscale_factor)), resample=Image.BICUBIC)\n",
        "          lr_image_full.save(os.path.join(test_root, 'lr', lr_image_name))\n",
        "\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "avg /= len(files)\n",
        "print(f'Average PSNR over all images: {avg:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RspqEXqwneCL",
        "outputId": "0a58e9d0-f246-4a7b-8e5c-c31e5738e163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0807.png, PSNR: 19.855\n",
            "0811.png, PSNR: 28.675\n",
            "average psnr for 2: 24.2651\n",
            "0807.png, PSNR: 18.409\n",
            "0811.png, PSNR: 24.519\n",
            "average psnr for 4: 21.4643\n"
          ]
        }
      ],
      "source": [
        "# For small image testing\n",
        "\n",
        "import os\n",
        "\n",
        "#################\n",
        "upscale_factors = [2,4]\n",
        "#################\n",
        "\n",
        "test_root = os.path.join(drive_root,'dataset/test')\n",
        "files = os.listdir(os.path.join(test_root, 'origin'))\n",
        "torch.cuda.empty_cache()\n",
        "# model = Generator().to(device)\n",
        "# model.load_state_dict(torch.load(os.path.join(drive_root, \"model/dis_train1/model_50.pth\") ,weights_only=True))\n",
        "g_model.eval()\n",
        "\n",
        "\n",
        "\n",
        "for u in upscale_factors :\n",
        "    avg = 0\n",
        "    for file in files:\n",
        "        # if file != \"ss1.png\": continue\n",
        "        with Image.open(os.path.join(test_root, 'origin', file)) as hr_image:\n",
        "            lr_shape = (int(math.floor(hr_image.width/u)),\n",
        "                            int(math.floor(hr_image.height/u)))\n",
        "\n",
        "            hr_shape = (int(u * lr_shape[0]),\n",
        "                                int(u * lr_shape[1]))\n",
        "\n",
        "            hr_image = hr_image.crop((0, 0, hr_shape[0], hr_shape[1]))\n",
        "            lr_image = hr_image.resize(lr_shape, resample=Image.BICUBIC)\n",
        "\n",
        "            hr = np.array(asarray(hr_image)).astype(np.float32).transpose((2, 0, 1))/255.\n",
        "            hr = np.expand_dims(hr, 0)\n",
        "\n",
        "            lr = np.array(asarray(lr_image)).astype(np.float32).transpose((2, 0, 1))/255.\n",
        "            lr = np.expand_dims(lr, 0)\n",
        "\n",
        "            lr = lr[:, :3, :, :]##\n",
        "\n",
        "            hr = torch.from_numpy(hr).to(device, non_blocking=True)\n",
        "            lr = torch.from_numpy(lr).to(device, non_blocking=True)\n",
        "\n",
        "            batch_size, channels, lr_height, lr_width = lr.size()\n",
        "            _, _, hr_height, hr_width = hr.size()\n",
        "            pos_matrix, mask_matrix = weight_prediction_matrix_from_lr(lr_height, lr_width, u)\n",
        "            pos_matrix = pos_matrix.to(device, non_blocking=True)\n",
        "            mask_matrix = mask_matrix.to(device, non_blocking=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sr = g_model(lr, pos_matrix, u)\n",
        "                sr = torch.masked_select(sr, mask_matrix)\n",
        "                sr = sr.contiguous().view(batch_size, channels, hr_height, hr_width)\n",
        "\n",
        "            psnr = psnr_between_rgb(hr, sr)\n",
        "            avg += psnr\n",
        "            print(f'{file}, PSNR: {psnr:.3f}')\n",
        "\n",
        "            # save image\n",
        "            sr = sr.mul(255.0).cpu().numpy()\n",
        "            sr = np.clip(sr, 0.0,255.0).astype(np.uint8)\n",
        "            sr = sr.squeeze().transpose((1,2,0))\n",
        "            sr_image = Image.fromarray(sr)\n",
        "\n",
        "            sr_image.save(os.path.join(test_root, 'sr', str(u) + \"_\" + file))\n",
        "            hr_image.save(os.path.join(test_root, 'hr', file))\n",
        "            lr_image.save(os.path.join(test_root,'lr', file))\n",
        "            del hr, lr, pos_matrix, mask_matrix, sr, sr_image, hr_image, lr_image, _, lr_shape, hr_shape\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    g_model.train()\n",
        "\n",
        "    avg /= len(files)\n",
        "    print(f'average psnr for {u}: {avg:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
